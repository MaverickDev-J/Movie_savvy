# The path to the base model's checkpoint directory to load for finetuning.
checkpoint_dir: checkpoints/mistralai/Mistral-7B-v0.1

# Directory in which to save checkpoints and logs.
out_dir: out/qlora-mistral-7b-custom-data

# The precision to use for finetuning.
precision: bf16-true

# Quantize the model with this algorithm.
quantize: bnb.nf4-dq

# How many devices/GPUs to use.
devices: 1

# LoRA configuration
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_query: true
lora_key: false
lora_value: true
lora_projection: false
lora_mlp: false
lora_head: false

# Data-related arguments - UPDATED FOR YOUR CUSTOM DATA
# Data-related arguments - CORRECTED
data:
  class_path: litgpt.data.JSON
  init_args:
    json_path: data/custom
    prompt_style: alpaca
    mask_prompt: false
    ignore_index: -100
    seed: 42
    num_workers: 4

# Training-related arguments - OPTIMIZED FOR YOUR DATASET SIZE
train:
  save_interval: 500
  log_interval: 10
  global_batch_size: 16
  micro_batch_size: 4
  lr_warmup_steps: 100
  epochs: 1  # With 232k samples, 1 epoch should be sufficient
  max_seq_length: 1024  # Increased for longer movie conversations
  learning_rate: 0.0001
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  min_lr: 1.0e-05

# Evaluation-related arguments
eval:
  interval: 500
  max_new_tokens: 150  # More tokens for detailed movie discussions
  max_iters: 50

# Logger
logger_name: tensorboard

# Random seed
seed: 1337